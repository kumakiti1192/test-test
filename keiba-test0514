#!/usr/bin/env python
# coding: utf-8
# ============================================================
#  JRA ç«¶é¦¬äºˆæ¸¬ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³  â€•  ãƒ•ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ç‰ˆ 2025-05-13
#    * æ­£è¦è¡¨ç¾ 4 ç®‡æ‰€å¼·åŒ–
#    * loky ä¾‹å¤–ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ã
#    * CPU ä½¿ç”¨ç‡ = CPU_RATIO (%)
# ============================================================
_uc_driver_cached = None 
# ----------------- Imports ----------------------------------
# ----------------- Imports ----------------------------------
import os, re, time, random, logging, warnings, signal, subprocess, platform, pickle, datetime as dt
from pathlib import Path
from typing import Sequence, Callable, Optional
import numpy as np
import pandas as pd
from tqdm.notebook import tqdm                       # â† æ—¢å­˜
from bs4 import BeautifulSoup
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui  import WebDriverWait
from selenium.webdriver.support      import expected_conditions as EC
from selenium.common.exceptions      import TimeoutException
from concurrent.futures import ThreadPoolExecutor, as_completed
from concurrent.futures import ProcessPoolExecutor    # â˜… è¿½åŠ 
from joblib import Parallel, delayed
import lightgbm as lgb
from queue import SimpleQueue
import multiprocessing as mp
from contextlib import contextmanager
from joblib import Memory

import multiprocessing as mp
if mp.get_start_method(allow_none=True) != "spawn":
    mp.set_start_method("spawn", force=True)

memory = Memory("cache/horse_parse", verbose=0)


# ----------------- åŸºæœ¬è¨­å®š --------------------------------
TRAIN_MODE  = True                       # False ã§ PREDICT
ANSWER_CHECK_ENABLED = True
FROM_YYYMMM = "2024-01"
TO_YYYMMM   = "2025-04"
RACE_DATE   = "20250427"
CPU_RATIO   = 0.8                        # ä¸¦åˆ—åº¦ï¼CPUÃ—80%

DIR_HTML_RACE   = Path("html/race")
DIR_HTML_HORSE  = Path("html/horse")
DIR_HTML_RESULT = Path("html/race_result")
DIR_DATA        = Path("data")
DIR_MODEL       = Path("models")
DIR_OUTPUT      = Path("output")

PKL_RESULT      = DIR_DATA / "df_result.pkl"
PKL_HORSE_ALL   = DIR_DATA / "df_horse_all.pkl"
PKL_PAST5       = DIR_DATA / "df_past5.pkl"
PKL_JOCKEY      = DIR_DATA / "df_jockey.pkl"
MODEL_FILE      = DIR_MODEL / "model_jra.txt"
RESULT_HTML_DIR = DIR_HTML_RESULT            # æ—¢å­˜ã¨çµ±ä¸€
DIST_CAT_BINS   = [0, 1400, 1800, 2200, 3000, 9999]
DIST_CAT_LABELS = ["S", "M", "IM", "L", "EL"]

# ----------------- ãƒ­ã‚®ãƒ³ã‚° --------------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)
uc.logger.setLevel(logging.ERROR)
warnings.filterwarnings("ignore", message=".*could not detect.*")

# ----------------- tqdm ãƒ©ãƒƒãƒ‘ãƒ¼ï¼ˆå†å¸°ãƒã‚°ä¿®æ­£ï¼‰ -------------
def tqdm_notebook(*args, **kwargs):
    """
    tqdm.notebook ã‚’ `disable=False` å›ºå®šã§ãƒ©ãƒƒãƒ—ã€‚
    ä¾‹) for x in tqdm_notebook(iterable, desc="èª¬æ˜"):
    """
    kwargs.setdefault("disable", False)
    return tqdm(*args, **kwargs)        # â˜… è‡ªåˆ†è‡ªèº«ã‚’å‘¼ã°ãªã„

# ============================================================
#  PART-1 : Chrome Driver / GET / HTML ä¿å­˜ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ============================================================
UA_POOL = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/123.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 13_2) AppleWebKit/605.1.15 Version/17.0 Safari/605.1.15",
    "Mozilla/5.0 (Linux; Android 13; Pixel 7) AppleWebKit/537.36 Chrome/124.0.0.0 Mobile Safari/537.36",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 17_2 like Mac OS X) AppleWebKit/605.1.15 Version/17.0 Mobile Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 ",
    "(KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36 Edg/124.0.0.0",
]
def random_ua() -> str: return random.choice(UA_POOL)

def kill_zombie_chrome():
    if platform.system() != "Windows":
        return
    try:
        out = subprocess.check_output("tasklist").decode(errors="ignore")
        for line in out.splitlines():
            if re.search(r"(chrome|chromedriver|msedge)", line, re.I):
                os.kill(int(line.split()[1]), signal.SIGTERM)
    except Exception:
        pass


def prepare_chrome_driver(
    *, headless: bool=True, no_images: bool=False,
    version_main: int=135, max_retry: int=4
):
    def _kill(pattern=r"(chrome|chromedriver)"):
        if platform.system() != "Windows": return
        try:
            out = subprocess.check_output("tasklist").decode(errors="ignore")
            for ln in out.splitlines():
                if re.search(pattern, ln, re.I):
                    os.kill(int(ln.split()[1]), signal.SIGTERM)
        except Exception: pass

    global _uc_driver_cached
    for att in range(1, max_retry+1):
        try:
            _kill(); time.sleep(0.8)
            opt = uc.ChromeOptions()
            opt.add_argument(f"--user-agent={random_ua()}")
            if headless: opt.add_argument("--headless=new")
            for flag in ("--disable-gpu","--no-sandbox","--disable-dev-shm-usage",
                         "--disable-blink-features=AutomationControlled","--remote-debugging-port=0"):
                opt.add_argument(flag)
            if no_images:
                opt.add_experimental_option("prefs",{"profile.managed_default_content_settings.images":2})
            kw = dict(options=opt, patcher_force_close=True,
                      version_main=version_main, log_level=3, timeout=90)
            if _uc_driver_cached:
                kw["driver_executable_path"] = str(_uc_driver_cached)
            driver = uc.Chrome(**kw)
            if _uc_driver_cached is None:
                _uc_driver_cached = Path(driver.service.path).resolve()
            driver.execute_cdp_cmd(
                "Page.addScriptToEvaluateOnNewDocument",
                {"source": "Object.defineProperty(navigator,'webdriver',{get:()=>undefined});"}
            )
            logging.info("[Driver] Chrome èµ·å‹•æˆåŠŸ")
            return driver
        except Exception as e:
            logging.warning(f"[Driver Retry {att}] {e}")
            _kill(); time.sleep(2+random.random())
    raise RuntimeError("Chrome èµ·å‹•å¤±æ•—")

NETWORK_IDLE_JS = """
return (function(sec){
  const now=performance.now();
  const recent=performance.getEntriesByType('resource')
               .filter(r=>(now-r.responseEnd)<(sec*1000));
  return recent.length===0;})(arguments[0]);
"""

def safe_driver_get(
    driver, url:str, *, wait_xpath:str|None=None,
    timeout:int=45, quiet_sec:int=2, max_retry:int=3,
    extra_wait:Optional[Callable]=None
) -> bool:
    for att in range(1,max_retry+1):
        try:
            driver.set_page_load_timeout(timeout)
            driver.get(url)
            WebDriverWait(driver, timeout).until(lambda d: d.execute_script("return document.readyState")=="complete")
            if wait_xpath:
                WebDriverWait(driver, timeout).until(EC.presence_of_element_located((By.XPATH,wait_xpath)))
            if extra_wait:
                WebDriverWait(driver, timeout).until(extra_wait)
            WebDriverWait(driver, timeout).until(lambda d: d.execute_script(NETWORK_IDLE_JS, quiet_sec))
            return True
        except Exception as e:
            logging.warning(f"[GET-Retry {att}/{max_retry}] {url} â†’ {e.__class__.__name__}")
            time.sleep(2+random.random())
    return False

def safe_driver_get_multi(
    driver, url:str, *, wait_xpaths:Sequence[str],
    timeout:int=45, max_retry:int=3, extra_wait:Optional[Callable]=None
) -> bool:
    cond = EC.any_of(*[EC.presence_of_element_located((By.XPATH,x)) for x in wait_xpaths])
    for att in range(1,max_retry+1):
        try:
            driver.set_page_load_timeout(timeout); driver.get(url)
            WebDriverWait(driver, timeout).until(cond)
            if extra_wait: WebDriverWait(driver, timeout).until(extra_wait)
            WebDriverWait(driver, timeout).until(lambda d: d.execute_script("return document.readyState")=="complete")
            return True
        except Exception as e:
            logging.warning(f"[GET-Retry {att}/{max_retry}] {url} â†’ {e.__class__.__name__}")
            time.sleep(2+random.random())
    return False

# -- â¶ æ­£è¦è¡¨ç¾â‘   looks_like_result_html  ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰&ãƒ†ãƒ¼ãƒ–ãƒ«æ¤œå‡º ---------
def looks_like_result_html(html: str) -> bool:
    if ("ç€é †" not in html) or ("é¦¬å" not in html):
        return False
    return ('id="All_Result_Table"' in html or 
            'class="RaceTable01"'   in html or    # æ—§ãƒ†ãƒ¼ãƒ–ãƒ«
            'class="RaceTable02"'   in html or    # æ–°ãƒ†ãƒ¼ãƒ–ãƒ«
            'class="ResultTableWrap"' in html)    # äºˆå‚™

def is_valid_result_html(path: Path, min_kb:int=50) -> bool:
    if not path.exists() or path.stat().st_size < min_kb*1024: return False
    try: return looks_like_result_html(path.read_text("utf-8","ignore"))
    except Exception: return False

def ensure_html(
    path:Path, url:str, *, wait_xpaths:Optional[Sequence[str]]=None,
    no_images:bool=False, timeout:int=60, max_retry:int=4
)->bool:
    if is_valid_result_html(path): return True
    if path.exists(): path.unlink(missing_ok=True)
    wait_xpaths = wait_xpaths or ['//*[@id="All_Result_Table"]',
                                  '//*[@class="RaceTable01"]',
                                  '//*[@class="RaceTable02"]',
                                  '//*[@class="ResultTableWrap"]']
    for att in range(1,max_retry+1):
        drv = prepare_chrome_driver(headless=True,no_images=no_images)
        ok  = safe_driver_get_multi(drv,url,wait_xpaths=wait_xpaths,
                                    timeout=timeout,max_retry=1)
        html=drv.page_source if ok else ""; drv.quit()
        if ok and looks_like_result_html(html):
            path.write_bytes(html.encode("utf-8")); return True
        logging.warning(f"[ensure] retry {att}/{max_retry} â†’ {url}")
    logging.error(f"[ensure] GIVE-UP: {url}"); return False

def save_pickle_safe(obj,path:Path):
    tmp=path.with_suffix(".tmp"); pickle.dump(obj,tmp.open("wb")); tmp.replace(path)
def load_pickle_safe(path:Path):
    try:
        with open(path,"rb") as f: data=pickle.load(f)
        if isinstance(data,list) and len(data)%12!=0:
            logging.warning(f"[CACHE] {path.name} ç ´æ£„"); path.unlink(); return None
        return data
    except Exception:
        path.unlink(missing_ok=True); return None

# ============================================================
#  PART-2 : race_id / calendar / HTML å–å¾—
# ============================================================

# ---------- 1. race_id å–å¾— ---------------------------------
def scrape_race_ids_one_day(
        kaisai_date: str,
        cache_dir: Path = Path("cache/race_ids"),
        force_update: bool = False) -> list[str]:

    cache_dir.mkdir(parents=True, exist_ok=True)
    cfile = cache_dir / f"race_ids_{kaisai_date}.pkl"
    if not force_update and cfile.exists():
        if (ids := load_pickle_safe(cfile)) is not None:
            logging.info(f"[CACHE] {kaisai_date} race_id {len(ids)} ä»¶")
            return ids

    url = f"https://race.netkeiba.com/top/race_list.html?kaisai_date={kaisai_date}"
    r = requests.get(url,
                     headers={"User-Agent": "Mozilla/5.0"},
                     timeout=20)
    r.raise_for_status()

    soup = BeautifulSoup(r.text, "lxml")
    ids: list[str] = [
        m.group(1) for a in soup.select('.RaceList_DataWrap a[href*="race_id="]')
        if (m := re.search(r'race_id=(\d{12})', a["href"]))
    ]

    ids = sorted(set(ids))
    if len(ids) % 12 != 0:
        logging.warning(f"[race_list] {kaisai_date} â†’ {len(ids)} ä»¶ (ä¸å®Œå…¨) ç ´æ£„")
        return []

    save_pickle_safe(ids, cfile)
    logging.info(f"[race_list] {kaisai_date}: {len(ids)} ä»¶")
    return ids

# ---------- 2. é–‹å‚¬æ—¥ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ --------------------------------------------
def scrape_kaisai_date_selenium(from_yyyymm: str, to_yyyymm: str) -> list[str]:
    CAL = "https://race.netkeiba.com/top/calendar.html"
    rng = pd.date_range(f"{from_yyyymm}-01", f"{to_yyyymm}-28", freq="M")
    driver = prepare_chrome_driver(headless=True, no_images=True)
    dates: list[str] = []
    for y, m in tqdm_notebook(zip(rng.year, rng.month), total=len(rng), desc="calendar"):
        driver.get(f"{CAL}?year={y}&month={m:02d}")
        try:
            WebDriverWait(driver, 15).until(
                EC.presence_of_element_located((By.CLASS_NAME, "Calendar_Table"))
            )
            for a in driver.find_elements(By.XPATH,
                    '//table[@class="Calendar_Table"]//a'):
                href = a.get_attribute("href") or ""
                m = re.search(r"kaisai_date=(\d{8})", href)
                if m: dates.append(m.group(1))
        except Exception: pass
    driver.quit()
    dates = sorted(set(dates))
    logging.info(f"[é–‹å‚¬æ—¥] {len(dates)} æ—¥")
    return dates

# ============================================================
# 3. RESULT HTML ãƒãƒ«ã‚¯å–å¾—
# ============================================================
def _need_download(rid: str) -> bool:
    f = DIR_HTML_RESULT / f"race_{rid}.bin"
    try:
        return (not f.exists()) or f.stat().st_size < 50_000
    except Exception:
        return True


def fetch_result_html_bulk(race_ids: list[str], max_workers: int = 12) -> None:
    need_dl: list[str] = []
    with ThreadPoolExecutor(max_workers=max_workers) as exe:
        fut = {exe.submit(_need_download, rid): rid for rid in race_ids}
        for f in tqdm_notebook(as_completed(fut), total=len(race_ids),
                      desc="integrity", leave=False):
            if f.result():
                need_dl.append(fut[f])

    ok_cnt = len(race_ids) - len(need_dl)
    logging.info(f"[RESULT HTML] DL äºˆå®š = {len(need_dl)} / OK = {ok_cnt}")
    if not need_dl:
        return

    for rid in tqdm_notebook(need_dl, desc="download result html"):
        ensure_html(
            path=DIR_HTML_RESULT / f"race_{rid}.bin",
            url=f"https://race.netkeiba.com/race/result.html?race_id={rid}",
            wait_xpaths=['//*[@id="All_Result_Table"]',
                         '//*[@class="RaceTable01"]',
                         '//*[@class="RaceTable02"]',
                         '//*[@class="ResultTableWrap"]'],
            no_images=True
        )

# ============================================================
# result.html â†’ DataFrame  â€•  v4  (ç©ºç™½é™¤å»ã§åˆ—åã‚’æ­£è¦åŒ–)
# ============================================================
def parse_result_html(path: Path) -> pd.DataFrame:
    """
    race_XXXXXXXXXXXX.bin â†’ æˆç¸¾ DataFrame

    ãƒ»MultiIndex ãƒ˜ãƒƒãƒ€ã§ ('ç€','é †') ãªã©åˆ†å‰²ã•ã‚Œã¦ã‚‚ç€é †åˆ—ã‚’èªè­˜
    ãƒ»åˆ—ãƒ©ãƒ™ãƒ«ã®å…¨è§’ï¼åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã‚’ã™ã¹ã¦é™¤å»ã—ã¦æ­£è¦åŒ–
    ãƒ»ã€Œé¦¬ç•ªã€åˆ—ãŒç„¡ã‘ã‚Œã° 3 åˆ—ç›®ã‚’è£œå®Œï¼ˆå…¨è§’æ•°å­—â†’åŠè§’å¤‰æ›ä»˜ãï¼‰
    """
    if not path.is_file():
        return pd.DataFrame()

    html = path.read_text("utf-8", "ignore")
    soup = BeautifulSoup(html, "lxml")

    # ---------- ãƒ†ãƒ¼ãƒ–ãƒ«å–å¾— ------------------------------------
    tbl = (soup.select_one("#All_Result_Table") or
           soup.select_one("table.Result_Table") or
           soup.select_one("table.RaceTable01")  or
           soup.select_one("table.RaceTable02"))
    if tbl is None:
        for t in soup.find_all("table"):
            if "ç€" in t.get_text() and "é †" in t.get_text():
                tbl = t
                break
    if tbl is None:
        return pd.DataFrame()

    # ---------- ãƒ˜ãƒƒãƒ€å€™è£œã‚’ç·å½“ãŸã‚Š -----------------------------
    def _has_chakujun(df: pd.DataFrame) -> bool:
        for col in df.columns:
            label = "".join(map(str, col)) if isinstance(col, tuple) else str(col)
            if ("ç€" in label) and ("é †" in label):
                return True
        return False

    df = None
    for hdr in (None, 0, 1):
        try:
            cand = pd.read_html(str(tbl), flavor="bs4", header=hdr)[0]
            if _has_chakujun(cand):
                df = cand
                break
        except Exception:
            continue
    if df is None:
        return pd.DataFrame()

    # ---------- â˜… åˆ—ãƒ©ãƒ™ãƒ«ã‚’æ­£è¦åŒ–ï¼ˆç©ºç™½å‰Šé™¤ï¼‰ --------------------
    def _clean_label(col) -> str:
        s = "".join(map(str, col)) if isinstance(col, tuple) else str(col)
        return re.sub(r"\s+", "", s)

    df.columns = [_clean_label(c) for c in df.columns]

    # ---------- é¦¬ç•ªè£œå®Œï¼ˆ3 åˆ—ç›®ã‚’è§£æï¼‰ -------------------------
    if "é¦¬ç•ª" not in df.columns and df.shape[1] >= 3:
        umaban = []
        for tr in BeautifulSoup(str(tbl), "lxml").select("tr")[1:]:
            tds = tr.find_all(["td", "th"])
            txt = re.sub(r"\s+", "", tds[2].get_text()) if len(tds) >= 3 else ""
            txt = txt.translate(str.maketrans("ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™", "0123456789"))
            m = re.match(r"^\d{1,2}$", txt)
            umaban.append(int(m.group()) if m else None)
        if len(umaban) == len(df):
            df.insert(2, "é¦¬ç•ª", umaban)

    # ---------- ID æŠ½å‡º ------------------------------------------
    hids = re.findall(r"/horse/(\d{5,10})", str(tbl))
    jids = re.findall(r"/jockey/(?:result/[^/]+/|profile/|detail/)?(\d+)", str(tbl))
    n = len(df)
    hids += [None] * (n - len(hids))
    jids += [None] * (n - len(jids))

    df = df.iloc[:n].copy()
    df["horse_id"]  = hids[:n]
    df["jockey_id"] = jids[:n]
    df["race_id"]   = path.stem[5:17]          # race_YYYYMMDDHHMM

    # ---------- æ•°å€¤åˆ—ã¸å¤‰æ› -------------------------------------
    for col in ("ç€é †", "æ ç•ª", "é¦¬ç•ª", "äººæ°—"):
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce")

    # ---------- åˆ—ãƒ•ã‚£ãƒ«ã‚¿ ---------------------------------------
    keep = ["race_id", "horse_id", "jockey_id",
            "ç€é †", "æ ç•ª", "é¦¬ç•ª", "äººæ°—"]
    return df[[c for c in keep if c in df.columns]]


# ============================================================
# 5. ãƒ—ãƒ­ã‚»ã‚¹ä¸¦åˆ—ãƒ‘ãƒ¼ã‚¹  (å …ç‰¢ç‰ˆ ï¼‹ fallback)
# ============================================================

class SkipThisRace(Exception):
    """æˆç¸¾ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå¾—ã‚‰ã‚Œãšã‚¹ã‚­ãƒƒãƒ—ã—ãŸã„å ´åˆã«é€å‡º"""

def parse_one_result_html(file_path: str | Path) -> pd.DataFrame:
    """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‘ãƒ¼ã‚¹ã€‚ãƒ‡ãƒ¼ã‚¿ãŒç„¡ã‘ã‚Œã° SkipThisRace"""
    df = parse_result_html(Path(file_path))
    if df.empty:
        raise SkipThisRace
    return df

# ============================================================
# 5. ãƒ—ãƒ­ã‚»ã‚¹ä¸¦åˆ—ãƒ‘ãƒ¼ã‚¹  (å …ç‰¢ç‰ˆï¼‹futureâ†’rid ãƒãƒƒãƒ”ãƒ³ã‚°ä¿®æ­£ç‰ˆ)
# ============================================================
def parse_result_html_bulk_loky(race_ids, html_dir, max_workers=None):
    """
    race_ids ã«å¯¾å¿œã™ã‚‹ result.html ã‚’ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰ã§ãƒ‘ãƒ¼ã‚¹ã€‚
    - å¤±æ•—ã—ãŸ race_id ã¯ skipped ã«å…¥ã‚Œã‚‹
    - NameError å›é¿ã®ãŸã‚ futureâ†’race_id ãƒãƒƒãƒ—ã‚’ä¿æŒ
    """
    def _one(rid: str) -> pd.DataFrame | None:
        file = os.path.join(html_dir, f"race_{rid}.bin")
        try:
            return parse_one_result_html(file)
        except SkipThisRace:
            return None

    ok_df, skipped = [], []
    max_workers = max_workers or os.cpu_count()

    with ThreadPoolExecutor(max_workers=max_workers) as ex, \
         tqdm_notebook(total=len(race_ids), desc="result-parse", unit="R") as bar:

        future_map = {ex.submit(_one, rid): rid for rid in race_ids}

        for fut in as_completed(future_map):
            rid = future_map[fut]                     # â† ã“ã‚Œã§ NameError è§£æ¶ˆ
            bar.update()
            df = fut.result()
            if df is None:
                skipped.append(rid)
            else:
                ok_df.append(df)

    return pd.concat(ok_df, ignore_index=True) if ok_df else pd.DataFrame(), skipped


# ============================================================
# 6. horse HTML å–å¾— & ãƒ‘ãƒ¼ã‚¹
# ============================================================
def is_valid_horse_html(path: Path,
                        min_size: int = 5_000,
                        must_have: tuple[str, ...] = ("ç€é †",)):
    if not path.is_file(): return False
    if path.stat().st_size < min_size:
        logging.debug(f"[horse-html] too small {path.name}")
        return False
    try:
        html = path.read_text("utf-8", "ignore")
    except Exception:
        return False
    compact = html.replace(" ", "").replace("ã€€", "")
    return all(k.replace(" ", "").replace("ã€€", "") in compact for k in must_have)

def download_horse_page(driver, horse_id: str, retries: int = 1) -> bool:
    fpath = DIR_HTML_HORSE / f"horse_{horse_id}.bin"
    if is_valid_horse_html(fpath):
        return True
    url = f"https://db.netkeiba.com/horse/{horse_id}/"
    ok = safe_driver_get(driver, url, wait_xpath='//*[@id="db_main_box"]',
                         timeout=60, max_retry=retries)
    if not ok:
        return False
    html = driver.page_source
    if "é¦¬å" not in html:
        return False
    fpath.write_bytes(html.encode("utf-8"))
    return True

from concurrent.futures import ThreadPoolExecutor

def scrape_horse_pages_bulk(horse_ids: list[str], retry_max: int = 2):
    """
    â‘  ãƒ­ãƒ¼ã‚«ãƒ« HTML ã®å­˜åœ¨ãƒ»å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯ã‚’ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰ã§é«˜é€ŸåŒ–
    â‘¡ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã ã‘ Selenium ã§å–å¾—
    """
    # ---------- â‘  æ—¢å­˜ãƒã‚§ãƒƒã‚¯ï¼ˆI/O ãƒã‚¦ãƒ³ãƒ‰ãªã®ã§ã‚¹ãƒ¬ãƒƒãƒ‰ä¸¦åˆ—ï¼‰ ----------
    def _need(hid: str) -> str | None:
        f = DIR_HTML_HORSE / f"horse_{hid}.bin"
        return hid if not is_valid_horse_html(f) else None

    # 32 ã‚¹ãƒ¬ãƒƒãƒ‰ç¨‹åº¦ãªã‚‰ CPU ä½¿ç”¨ç‡ã¯ã»ã¼å¢—ãˆãšã€ãƒ‡ã‚£ã‚¹ã‚¯ I/O ãŒå¤§å¹…çŸ­ç¸®
    with ThreadPoolExecutor(max_workers=16) as ex:
        targets = [hid for hid in ex.map(_need, horse_ids) if hid]

    ok_cnt = len(horse_ids) - len(targets)
    logging.info(f"[horse] ğŸ‘æ—¢å­˜OK = {ok_cnt:,}  /  DLäºˆå®š = {len(targets):,}")
    if not targets:
        return

    # ---------- â‘¡ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆå¾“æ¥ã©ãŠã‚Šå˜ä¸€ Chrome ã§é †æ¬¡ï¼‰ ----------
    driver = prepare_chrome_driver(headless=True, no_images=True)
    err = 0
    for hid in tqdm_notebook(targets, desc="horse"):
        if download_horse_page(driver, hid, retry_max):
            err = 0
        else:
            err += 1
            logging.warning(f"[horse] âŒ {hid}")
        if err >= 3:                          # 3 é€£ç¶šå¤±æ•—ã§ Chrome å†èµ·å‹•
            driver.quit()
            driver = prepare_chrome_driver(headless=True, no_images=True)
            err = 0
    driver.quit()


def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    colmap = {}
    for c in df.columns:
        cc = re.sub(r"\s+", "", str(c))
        if re.fullmatch(r"(æ—¥ä»˜|é–‹å‚¬æ—¥|å¹´æœˆæ—¥)", cc):      colmap[c] = "race_date"
        elif "ç€é †" in cc:                                colmap[c] = "ç€é †"
        elif re.search(r"(ä¸Šã‚Š|ä¸ŠãŒã‚Š).?3?F?", cc):       colmap[c] = "ä¸ŠãŒã‚Š3F"
        elif re.search(r"(é¦¬ä½“é‡|ä½“é‡)", cc):             colmap[c] = "é¦¬ä½“é‡"
        elif re.search(r"(é¦¬å ´|ã‚³ãƒ¼ã‚¹|ç¨®åˆ¥)", cc):        colmap[c] = "é¦¬å ´ç¨®åˆ¥"
        elif re.search(r"(é–‹å‚¬|ç«¶é¦¬å ´)", cc):             colmap[c] = "é–‹å‚¬"
        elif "race_id" in cc.lower():                     colmap[c] = "race_id"
    df = df.rename(columns=colmap)

    for col in ["race_date", "ç€é †", "ä¸ŠãŒã‚Š3F", "é¦¬ä½“é‡", "é¦¬å ´ç¨®åˆ¥"]:
        if col not in df.columns:
            df[col] = pd.NA

    df["race_date"] = pd.to_datetime(df["race_date"], errors="coerce")
    df["ç€é †"]      = pd.to_numeric(df["ç€é †"].astype(str).str.extract(r"(\d+)")[0],
                                 errors="coerce")
    df["ä¸ŠãŒã‚Š3F"]  = pd.to_numeric(df["ä¸ŠãŒã‚Š3F"].astype(str)
                                 .str.replace("ç§’", ""), errors="coerce")
    df["é¦¬ä½“é‡"]    = pd.to_numeric(df["é¦¬ä½“é‡"].astype(str)
                                 .str.extract(r"(\d+)")[0], errors="coerce")

    def _track(x):
        if isinstance(x, str):
            if "èŠ" in x: return "èŠ"
            if "ãƒ€" in x or "ç ‚" in x or "ãƒ€ãƒ¼ãƒˆ" in x: return "ãƒ€"
        return pd.NA
    df["é¦¬å ´ç¨®åˆ¥"] = df["é¦¬å ´ç¨®åˆ¥"].apply(_track)

    if "race_id" not in df.columns:
        df["race_id"] = pd.NA

    # === ğŸ†• è¿½åŠ ã“ã“ã‹ã‚‰ =================================================
    # åŒã˜åˆ—ãƒ©ãƒ™ãƒ«ãŒè¤‡æ•°ã‚ã‚‹å ´åˆã¯ã€å…ˆé ­åˆ—ã®ã¿æ®‹ã—ã¦é‡è¤‡ã‚’é™¤å»
    if df.columns.duplicated().any():
        df = df.loc[:, ~df.columns.duplicated()]
    # === ğŸ†• è¿½åŠ ã“ã“ã¾ã§ =================================================

    return df


# -------------------------------------------------------------
#  å˜ä¸€é¦¬ãƒšãƒ¼ã‚¸ HTML ãƒã‚¤ãƒˆåˆ— â†’ DataFrame  ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰
# -------------------------------------------------------------
# ==== ç½®æ›å¯¾è±¡: _parse_html_bytes_raw é–¢æ•° ====
def _parse_html_bytes_raw(html_bytes: bytes, hid: str) -> pd.DataFrame:
    """Horse-page <table> â†’ DataFrame   (æ”¹è¡Œå…¥ã‚Šãƒ˜ãƒƒãƒ€å®Œå…¨å¯¾å¿œç‰ˆ)"""
    soup = BeautifulSoup(html_bytes, "lxml")

    # 1) table æŠ½å‡º ----------------------------------------------------------------
    tbl = (soup.select_one("#All_Result_Table")
           or soup.select_one("table.RaceTable01")
           or soup.select_one("table.RaceTable02")
           or soup.select_one("table.db_h_race_results")       # â˜… main
           or soup.select_one("table.HorseRaceTable")
           or soup.select_one("table.HorseRaceTable01")
           or soup.select_one("table.HorseRaceTable02"))
    if tbl is None:
        raise ValueError("æˆç¸¾ãƒ†ãƒ¼ãƒ–ãƒ«æœªæ¤œå‡º")

    # 2) pandas.read_html â€• ãƒ˜ãƒƒãƒ€ç·å½“ãŸã‚Š -----------------------
    def has_chakujun(df: pd.DataFrame) -> bool:
        """åˆ—ã« â€˜ç€â€™ ã¨ â€˜é †â€™ ã®ä¸¡æ–¹ã‚’å«ã‚€ã‚»ãƒ«ãŒã‚ã‚Œã° OK"""
        for col in df.columns:
            txt = "".join(map(str, col)) if isinstance(col, tuple) else str(col)
            if "ç€" in txt and "é †" in txt:
                return True
        return False

    df_final = None
    for hdr in (None, 0, 1):
        for flavor in ("lxml", "bs4"):
            try:
                # â˜… match ã‚’å¤–ã—ã€å…¨ãƒ†ãƒ¼ãƒ–ãƒ«å–å¾—
                cand_list = pd.read_html(str(tbl), flavor=flavor, header=hdr)
            except ValueError:
                continue
            for cand in cand_list:
                if has_chakujun(cand) and cand.shape[0] >= 3:
                    df_final = cand
                    break
            if df_final is not None:
                break
        if df_final is not None:
            break

    if df_final is None:
        raise ValueError("ç€é †åˆ—ã‚’å«ã‚€ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè§£æã§ãã¾ã›ã‚“")

    # 3) é¦¬ç•ªåˆ—ãŒç„¡ã‘ã‚Œã° 3 åˆ—ç›®ã‚’è£œå®Œ ---------------------------
    if "é¦¬ç•ª" not in df_final.columns and df_final.shape[1] >= 3:
        nums = []
        for tr in tbl.find_all("tr")[1:]:
            tds = tr.find_all(["td", "th"])
            raw = tds[2].get_text(" ", strip=True) if len(tds) >= 3 else ""
            num = re.sub(r"\D", "", raw.translate(str.maketrans("ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™",
                                                                "0123456789")))
            nums.append(int(num) if num else None)
        if len(nums) == len(df_final):
            df_final.insert(2, "é¦¬ç•ª", nums)

    # 4) å…±é€šæ•´å½¢ ------------------------------------------------
    df_final = _normalize_columns(df_final)
    df_final["horse_id"] = hid
    return df_final


# ------------ ä¸€æ‹¬è§£æ -------------------------------------------------
def parse_horse_data_all(horse_ids: list[str],
                         max_workers: int | None = None) -> pd.DataFrame:
    """
    HTML ã‚’ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã§èª­ã¿ã€ãã®ãƒã‚¤ãƒˆåˆ—ã‚’ loky ãƒ—ãƒ­ã‚»ã‚¹ã¸æ¸¡ã™ã€‚
    ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„é¦¬ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ç¶šè¡Œã™ã‚‹ã€‚
    """
    # --- 1) HTML èª­ã¿è¾¼ã¿ (I/O) --------------------------------
    paths = [DIR_HTML_HORSE / f"horse_{hid}.bin" for hid in horse_ids]

    def _load_bytes(path: Path):
        if not is_valid_horse_html(path):
            raise FileNotFoundError(path.name)
        return path.read_bytes(), path.stem[6:]  # (bytes, horse_id)

    with ThreadPoolExecutor(max_workers=16) as ex:
        tasks = list(ex.map(_load_bytes, paths))

    # --- 2) å¤šãƒ—ãƒ­ã‚»ã‚¹ãƒ‘ãƒ¼ã‚¹ (CPU) ------------------------------
    max_workers = max_workers or max(1, int(os.cpu_count() * CPU_RATIO))
    logging.info(f"[horse-parse] fork workers = {max_workers}")

    # â˜… è¿½åŠ : ä¾‹å¤–ã‚’é£²ã¿è¾¼ã‚“ã§ None ã‚’è¿”ã™å®‰å…¨ãƒ©ãƒƒãƒ‘ãƒ¼ ----------
    def _safe_parse(html, hid):
        try:
            return _parse_html_bytes_raw(html, hid)   # å…ƒãƒ‘ãƒ¼ã‚µ
        except Exception as e:
            logging.debug(f"[horse-miss] {hid} â‡¢ {e}")
            return None                                # ã‚¹ã‚­ãƒƒãƒ—æ‰±ã„

    results = Parallel(
        n_jobs=max_workers, backend="loky", prefer="processes"
    )(
        delayed(_safe_parse)(html, hid)
        for html, hid in tqdm_notebook(tasks, desc="horse-parse", unit="é ­")
    )

    # â˜… å¤‰æ›´: OK/NG ã‚’æŒ¯ã‚Šåˆ†ã‘ã¦ NG ã¯è­¦å‘Šã®ã¿ -------------------
    ok = [df for df in results if isinstance(df, pd.DataFrame)]
    ng = len(results) - len(ok)
    if not ok:
        raise RuntimeError("å…¨é ­å¤±æ•—: æˆç¸¾ãƒ†ãƒ¼ãƒ–ãƒ«ãŒæ¤œå‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
    if ng:
        logging.warning(f"[horse] ãƒ†ãƒ¼ãƒ–ãƒ«æœªæ¤œå‡ºã‚¹ã‚­ãƒƒãƒ— = {ng} é ­")

    return pd.concat(ok, ignore_index=True)


# %% åŒºé–“ã‚¿ã‚¤ãƒ ãƒ»è„šè³ªç‰¹å¾´é‡  -------------------------------

# =========================  race passing-order â†’ è„šè³ªåˆ†é¡  =========================
def classify_style_by_passing(passing: str, head: int) -> str | None:
    """
    é€šéé †æ–‡å­—åˆ—ï¼ˆä¾‹: '1-1-1-2' ã‚„ '5-5-3-1'ï¼‰ã¨é ­æ•°ã‹ã‚‰
    ['é€ƒã’','é€ƒã’ãƒãƒ†','å…ˆè¡Œ','å·®ã—','è¿½è¾¼'] ã®ã„ãšã‚Œã‹ã‚’è¿”ã™
      * head : ãã®ãƒ¬ãƒ¼ã‚¹ã®å‡ºèµ°é ­æ•°
    æˆ»ã‚Šå€¤ãŒ None ã®ã¨ãã¯åˆ¤å®šä¿ç•™
    """
    if not passing or head <= 0:
        return None

    # â‘  4 åŒºé–“ã®é †ä½ãƒªã‚¹ãƒˆã‚’ç¢ºä¿ï¼ˆæ¬ æã¯æœ«å°¾å€¤ã§åŸ‹ã‚ã‚‹ï¼‰
    pos = [int(p) for p in re.split(r"[^\d]+", passing) if p.isdigit()]
    if not pos:
        return None
    pos = (pos + [pos[-1]] * 4)[:4]              # è¦ç´ ä¸è¶³ã‚’è£œå®Œ
    p1, p2, p3, p4 = pos

    # â‘¡ å‰²åˆã«ç›´ã—ã¦ãƒ­ã‚¸ãƒƒã‚¯åˆ¤å®š
    r1, r24, r4 = p1 / head, (p2 + p3) / 2 / head, p4 / head
    if r1 <= 0.10:                                # ç™ºé¦¬ã§å…ˆé ­ 10 % ä»¥å†…
        return "é€ƒã’"       if r4 <= 0.50 else "é€ƒã’ãƒãƒ†"
    if r1 <= 0.25 and r4 <= 0.40:
        return "å…ˆè¡Œ"
    if r4 < r1:
        return "å·®ã—"
    if r1 >= 0.66 and r4 >= 0.66:
        return "è¿½è¾¼"
    return None


def _load_result_html(rid: str) -> str:
    path = RESULT_HTML_DIR / f"race_{rid}.bin"
    return path.read_text("utf-8", "ignore")

def _parse_section(html: str, umaban: int):
    """
    1 ãƒ¬ãƒ¼ã‚¹ HTML ã‹ã‚‰
      ãƒ»ãã®é¦¬ã®ä¸ŠãŒã‚Š 3F é †ä½ (rk)
      ãƒ»è„šè³ªãƒ©ãƒ™ãƒ« (st)
    ã‚’æŠ½å‡ºã—ã¦è¿”ã™ã€‚å¤±æ•—æ™‚ã¯ (None, None)ã€‚
    â˜… passingâ€order è§£æã‚’è¿½åŠ 
    """
    soup = BeautifulSoup(html, "lxml")
    tbl  = (soup.select_one("#All_Result_Table")
            or soup.select_one("table.RaceTable01")
            or soup.select_one("table.RaceTable02"))
    if tbl is None:
        return None, None

    # ---------- DataFrame åŒ– ----------  
    try:
        df = pd.read_html(str(tbl), flavor="lxml", header=0)[0]
    except Exception:
        return None, None

    # é ­æ•°ãƒ»å¯¾è±¡è¡Œ
    head = len(df)
    row  = df[df.get("é¦¬ç•ª").astype(str) == str(umaban)]
    if row.empty:
        return None, None

    # ---------- ä¸ŠãŒã‚Š 3F é †ä½ ----------  
    if "ä¸Šã‚Š" in df.columns:
        df["ä¸ŠãŒã‚Šé †"] = pd.to_numeric(df["ä¸Šã‚Š"], errors="coerce").rank(method="min")
        rk = int(row["ä¸ŠãŒã‚Šé †"].iloc[0]) if pd.notna(row["ä¸ŠãŒã‚Šé †"].iloc[0]) else None
    else:
        rk = None

    # ---------- è„šè³ªåˆ¤å®šï¼ˆpassing order æ–‡å­—åˆ—ï¼‰ ----------  
    passing_col = None
    for cand in ("é€šé", "ã‚³ãƒ¼ãƒŠãƒ¼é€šéé †"):      # è¡¨è¨˜ã‚†ã‚Œå¯¾å¿œ
        if cand in df.columns:
            passing_col = cand; break
    st = None
    if passing_col:
        passing = str(row[passing_col].iloc[0])
        st = classify_style_by_passing(passing, head)

    return rk, st


# -----------------------------------------------------------
# collect_section_features  â€• 0 ä»¶ãªã‚‰ç©º DF ã‚’è¿”ã—ã¦è­¦å‘Š
# -----------------------------------------------------------
def collect_section_features(df_hist: pd.DataFrame,
                             umaban_map: dict[tuple[str, str], int],
                             workers: int = 8) -> pd.DataFrame:
    """
    éå» 5 èµ°ã® â€œä¸ŠãŒã‚Š 3F é †ä½ & è„šè³ªâ€ ã‚’å–ã‚Šã¾ã¨ã‚ã¦ç‰¹å¾´é‡åŒ–ã€‚
      umaban_map : {(race_id, horse_id) : é¦¬ç•ª}
    """
    feats = []
    total = skipped_no_map = failed_parse = 0

    for hid, sub in tqdm_notebook(df_hist.groupby("horse_id"),
                                  desc="[section] horses"):
        sub = sub.sort_values("race_date", ascending=False).head(5)

        # ----- race_id & é¦¬ç•ª ãŒæƒã†ã‚‚ã®ã ã‘æŠ½å‡º ----------------
        tasks = []
        for rid in sub["race_id"]:
            key = (str(rid), str(hid))
            umaban = umaban_map.get(key)
            if umaban is not None:
                tasks.append((rid, umaban))
        if not tasks:
            skipped_no_map += 1
            continue

        total += 1
        ranks, styles = [], []
        with ThreadPoolExecutor(max_workers=workers) as ex:
            futs = {
                ex.submit(_parse_section, _load_result_html(rid), umaban): rid
                for rid, umaban in tasks
            }
            for f in as_completed(futs):
                try:
                    rk, st = f.result()
                    if rk is not None: ranks.append(rk)
                    if st is not None: styles.append(st)
                except Exception:
                    failed_parse += 1

        if not ranks:
            continue                                        # â† ä¸ŠãŒã‚Šç„¡ã—ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—

        arr = np.array(ranks)
        feats.append(dict(
            horse_id = hid,
            past5_section_rank_mean = arr.mean(),
            past5_section_rank_best = arr.min(),
            style_mode    = max(set(styles), key=styles.count) if styles else None,
            style_win_rate= (arr == 1).mean(),
            style_in3_rate= (arr <= 3).mean()
        ))

    if not feats:
        logging.error(f"[section] å…¨é¦¬å‡¦ç†æ¸ˆ={total} / mapãªã—={skipped_no_map} / "
                      f"parseå¤±æ•—={failed_parse}")
        raise RuntimeError("âŒ collect_section_features() â†’ æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ 0 ä»¶")

    return pd.DataFrame(feats)




# %% é¨æ‰‹Ã—ç«¶èµ°æ¡ä»¶é©æ€§  ------------------------------------
def build_jockey_course_stats(df_res: pd.DataFrame) -> pd.DataFrame:
    df = df_res.copy()
    df["dist_cat"] = pd.cut(df["distance"],
                            bins=DIST_CAT_BINS,
                            labels=DIST_CAT_LABELS)
    grp = df.groupby(["jockey_id", "dist_cat", "surface"], sort=False)
    rec = []
    for (jid, dcat, surf), sub in tqdm_notebook(grp,
                                               desc="[jockey-course]",
                                               total=grp.ngroups):
        fin = sub["ç€é †"].astype(int)
        rec.append(dict(jockey_id=jid, dist_cat=dcat, surface=surf,
                        jockey_course_rank_mean=fin.mean(),
                        jockey_course_in3_rate=(fin <= 3).mean()))
    return pd.DataFrame(rec)

# %% ãƒ©ãƒƒãƒ‘ãƒ¼ï¼ˆpast5 ã¨åŒåˆ—ã§ä¿å­˜ï¼‰ ------------------------
# %% ãƒ©ãƒƒãƒ‘ãƒ¼ï¼ˆpast5 ã¨åŒåˆ—ã§ä¿å­˜ï¼‰ ------------------------
PKL_SECTION = DIR_DATA / "df_section.pkl"

def _clean_umaban(series: pd.Series) -> pd.Series:
    """
    é¦¬ç•ªæ–‡å­—åˆ—ã‚’æ­£è¦åŒ–ã—ã¦ Int64 ã«å¤‰æ›
      - ç©ºç™½ãƒ»å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã‚’é™¤å»
      - å…¨è§’æ•°å­—â†’åŠè§’æ•°å­—
      - å…ˆé ­ã®æ•°å­—åˆ—ã ã‘æŠ½å‡ºï¼ˆä¾‹: ' 3 (ç¹°ä¸Š) ' â†’ 3ï¼‰
    """
    # å…ˆã«å…¨è§’â†’åŠè§’ã¸ä¸€æ‹¬å¤‰æ›
    table = str.maketrans("ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™", "0123456789")
    s = series.astype(str).str.translate(table)

    # ç©ºç™½é™¤å» â†’ æ•°å­—éƒ¨åˆ†æŠ½å‡º
    s = s.str.replace(r"\s+", "", regex=True)      # ã‚¹ãƒšãƒ¼ã‚¹å…¨å‰Šé™¤
    s = s.str.extract(r"(\d+)", expand=False)      # é€£ç¶šæ•°å­—ã‚’æŠœãå‡ºã™

    return pd.to_numeric(s, errors="coerce").astype("Int64")


# ============================================================
# calc_section_features  â€• é¦¬ç•ªãƒãƒƒãƒ—ã‚’ (race_id, horse_id) ã¸
# ============================================================
def calc_section_features(df_hist: pd.DataFrame,
                          df_result: pd.DataFrame) -> pd.DataFrame:
    """
    åŒºé–“ã‚¿ã‚¤ãƒ ãƒ»è„šè³ªç‰¹å¾´é‡ã‚’ç”Ÿæˆã€‚
    - é¦¬ç•ªã¯ result.html ç”±æ¥ï¼ˆrace_id+horse_id ã®å®Œå…¨ã‚­ãƒ¼ï¼‰ã‚’ä½¿ç”¨
    """
    # -- 1) å‰å‡¦ç† -------------------------------------------------
    df_result = (
        df_result
          .dropna(subset=["race_id", "horse_id", "é¦¬ç•ª"])
          .assign(race_id=lambda d: d["race_id"].astype(str).str.strip(),
                  horse_id=lambda d: d["horse_id"].astype(str).str.strip(),
                  é¦¬ç•ª=lambda d: pd.to_numeric(d["é¦¬ç•ª"], errors="coerce").astype("Int64"))
    )

    # -- 2) (race_id, horse_id) â†’ é¦¬ç•ª è¾æ›¸ ------------------------
    umaban_map: dict[tuple[str, str], int] = (
        df_result.set_index(["race_id", "horse_id"])["é¦¬ç•ª"].to_dict()
    )

    # -- 3) æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯å‘¼ã³å‡ºã— ----------------------------------
    return collect_section_features(df_hist, umaban_map)

# ============================================================
# 7. ç‰¹å¾´é‡ç”Ÿæˆ
# ============================================================
# -----------------------------------------------------------
# calc_section_features  â€•  horse ãƒšãƒ¼ã‚¸ãŒæ¬ æã—ã¦ã‚‚è½ã¡ãªã„ç‰ˆ
# -----------------------------------------------------------
def calc_section_features(df_hist: pd.DataFrame,
                          df_result: pd.DataFrame) -> pd.DataFrame:
    """
    åŒºé–“ã‚¿ã‚¤ãƒ ãƒ»è„šè³ªç‰¹å¾´é‡ã‚’ç”Ÿæˆã€‚
    å„ªå…ˆ: df_hist (horse ãƒšãƒ¼ã‚¸)  
    è£œå®Œ: df_result ã‹ã‚‰ race_id / horse_id ã‚’è¿½åŠ 
    """

    # --------- 1) è¶³ã‚Šãªã„ race_id ã‚’ df_result ã§è£œå®Œ ------------
    core_cols = {"horse_id", "race_id", "race_date"}
    if not core_cols.issubset(df_hist.columns):
        # df_hist ãŒã»ã¼ç©ºã®å ´åˆã‚‚æƒ³å®šã—ã¦ãƒŸãƒ‹ DF ã‚’ä½œã‚‹
        df_hist = df_hist.reindex(columns=list(core_cols)).copy()

    # result ã‹ã‚‰ race_date ã‚’æ¨å®šï¼ˆãªã„å ´åˆã¯ 2100-01-01 ã«ã—ã¦é™é † head(5) ãŒåŠ¹ãã‚ˆã†ã«ï¼‰
    add_cols = (df_result[["race_id", "horse_id"]]
                .dropna()
                .assign(race_date=lambda d:
                        pd.to_datetime(d["race_id"].str[:8], errors="coerce")
                        .fillna(pd.Timestamp("2100-01-01"))))
    df_hist_full = (pd.concat([df_hist[list(core_cols)], add_cols], ignore_index=True)
                      .drop_duplicates(subset=["horse_id", "race_id"]))

    # --------- 2) umaban_map  (rid, hid) â†’ é¦¬ç•ª ------------------
    umaban_map: dict[tuple[str, str], int] = (
        df_result.dropna(subset=["race_id", "horse_id", "é¦¬ç•ª"])
                 .assign(race_id=lambda d: d["race_id"].astype(str).str.strip(),
                         horse_id=lambda d: d["horse_id"].astype(str).str.strip(),
                         é¦¬ç•ª=lambda d: pd.to_numeric(d["é¦¬ç•ª"], errors="coerce"))
                 .dropna(subset=["é¦¬ç•ª"])
                 .set_index(["race_id", "horse_id"])["é¦¬ç•ª"]
                 .astype(int)
                 .to_dict()
    )

    # --------- 3) æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯å‘¼ã³å‡ºã— ---------------------------
    return collect_section_features(df_hist_full, umaban_map)



def calc_jockey_features(df_result: pd.DataFrame) -> pd.DataFrame:
    tmp = df_result[["jockey_id", "ç€é †"]].dropna()
    tmp["jockey_id"] = tmp["jockey_id"].astype(str)
    return (tmp.groupby("jockey_id", as_index=False)
              .agg(jockey_fukusho=("ç€é †", lambda s: (s <= 3).mean()),
                   jockey_count  =("ç€é †", "count")))

FEATS = [
    # --- past5 ---
    "past5_avg_rank", "past5_avg_3f", "past5_avg_weight",
    "past5_shiba_ratio",
    # --- åŒºé–“ã‚¿ã‚¤ãƒ  & è„šè³ª ---
    "past5_section_rank_mean", "past5_section_rank_best",
    "style_win_rate", "style_in3_rate",
    # --- é¨æ‰‹ ---
    "jockey_fukusho", "jockey_count",
    "jockey_course_rank_mean", "jockey_course_in3_rate"]

# ============================================================
# 8. LightGBM ãƒ¢ãƒ‡ãƒ«
# ============================================================
def train_lightgbm(df: pd.DataFrame):
    tr = df.sample(frac=0.8, random_state=42)
    va = df.drop(tr.index)
    dtr = lgb.Dataset(tr[FEATS], label=(tr["ç€é †"] <= 3).astype(int))
    dva = lgb.Dataset(va[FEATS], label=(va["ç€é †"] <= 3).astype(int))
    params = dict(objective="binary", metric="auc", learning_rate=0.05,
                  num_leaves=31, seed=42)
    logging.info("[LightGBM] training â€¦")
    model = lgb.train(params, dtr, 1000, valid_sets=[dtr, dva],
                      early_stopping_rounds=100, verbose_eval=200)
    DIR_MODEL.mkdir(exist_ok=True)
    model.save_model(str(MODEL_FILE), num_iteration=model.best_iteration)
    logging.info(f"[LightGBM] saved â†’ {MODEL_FILE}")
    return model

# ============================================================
# 9. æ‰•æˆ»ã—ãƒ‘ãƒ¼ã‚µ & ROI ãƒã‚§ãƒƒã‚¯
# ============================================================
def parse_wide_payoffs(path: Path) -> dict[str, int]:
    """
    çµæœ HTML ã‹ã‚‰ãƒ¯ã‚¤ãƒ‰æ‰•æˆ»ã—ã‚’æŠ½å‡ºã—ã¦ dict ã§è¿”ã™
      key = '02-05' ã®ã‚ˆã†ã« 2 æ¡ã‚¼ãƒ­åŸ‹ã‚ãƒ»æ˜‡é †
      val = æ‰•æˆ»ã—é‡‘é¡ï¼ˆå††, intï¼‰
    """
    if not path.exists():
        return {}

    soup = BeautifulSoup(path.read_bytes(), "lxml")

    # ------ â‘  <tr class="Wide"> ã‚’å„ªå…ˆçš„ã«å–å¾— ----------------
    tr = soup.find("tr", class_=re.compile(r"\bWide\b", re.I))
    if tr is None:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šclass åãŒç„¡ã„å ´åˆã¯ <div class="Result_Pay_Back">
        pay_div = soup.find("div", class_=re.compile(r"Result_Pay_Back", re.I))
        if not pay_div:
            return {}
        # ãƒ¯ã‚¤ãƒ‰è¡Œã‚’æ¢ã™
        for cand in pay_div.find_all("tr"):
            if "ãƒ¯ã‚¤ãƒ‰" in cand.get_text():
                tr = cand
                break
        if tr is None:
            return {}

    text = tr.get_text(" ", strip=True)

    # ------ â‘¡ ã€Œæ•°å­— æ•°å­— é‡‘é¡å††ã€ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é †ã«æŠ½å‡º ----------
    pattern = r"(\d+)\s+(\d+)\s+(\d{2,7})å††"
    res: dict[str, int] = {}
    for a, b, pay in re.findall(pattern, text):
        key = "-".join(sorted([a.zfill(2), b.zfill(2)]))
        res[key] = int(pay.replace(",", ""))
    return res


def answer_check(df_pred: pd.DataFrame, bet: int = 100):
    """
    äºˆæ¸¬ DataFrame ã‹ã‚‰ãƒ¯ã‚¤ãƒ‰çš„ä¸­æ•°ã¨ ROI ã‚’è¨ˆç®—
      - ä¸Šä½2é ­ã®ã€Œé¦¬ç•ªã€(æ å†…ã®å‡ºèµ°ç•ªå·) ã‚’è²·ã†ã¨ä»®å®š
      - æ‰•æˆ»ã—è¾æ›¸ã¯ parse_wide_payoffs() ã§å–å¾—
    """
    if "é¦¬ç•ª" not in df_pred.columns:
        logging.warning("[ç­”ãˆåˆã‚ã›] df ã« 'é¦¬ç•ª' åˆ—ãŒã‚ã‚Šã¾ã›ã‚“")
        return

    pays = []
    for rid, sub in df_pred.groupby("race_id"):
        # â‘  ã‚¹ã‚³ã‚¢ä¸Šä½ 2 é ­ã®é¦¬ç•ªã‚’å–ã‚Šå‡ºã™
        nums = (sub.nlargest(2, "pred_score")["é¦¬ç•ª"]
                   .dropna().astype(int).astype(str)
                   .str.zfill(2)     # 2 æ¡ã‚¼ãƒ­åŸ‹ã‚
                   .tolist())
        if len(nums) < 2:
            continue

        key = "-".join(sorted(nums))         # ä¾‹: 02-05
        pay_dict = parse_wide_payoffs(DIR_HTML_RESULT / f"race_{rid}.bin")
        pays.append(pay_dict.get(key, 0))

    if not pays:
        logging.warning("[ç­”ãˆåˆã‚ã›] æ‰•æˆ»ã—ãƒ‡ãƒ¼ã‚¿ç„¡ã—")
        return

    hit = sum(1 for p in pays if p)
    roi = sum(pays) / (len(pays) * bet) * 100
    logging.info(f"[ç­”ãˆåˆã‚ã›] ãƒ¬ãƒ¼ã‚¹={len(pays)} çš„ä¸­={hit} ROI={roi:.1f}%")

# %%  TRAIN / PREDICT ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³  ------------------------------------
# %% =========================================================
#  TRAIN ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³  â€• Chrome ãƒ•ã‚§ãƒ¼ã‚ºã¨å¤šãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Œå…¨åˆ†é›¢
# ============================================================
def train_pipeline():
    logging.info("=== TRAIN MODE ===")
    kill_zombie_chrome()                        # æ®‹å­˜ãƒ—ãƒ­ã‚»ã‚¹æƒé™¤

    # ---------- â‘  é–‹å‚¬æ—¥ â†’ race_id åé›†  (Chrome ä½¿ç”¨) ----------
    dates = scrape_kaisai_date_selenium(FROM_YYYMMM, TO_YYYMMM)
    race_ids = sorted({rid for d in dates for rid in scrape_race_ids_one_day(d)})
    logging.info(f"[TRAIN] race_id = {len(race_ids):,}")

    # ---------- â‘¡ result.html ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰  (Chrome ä½¿ç”¨) -------
    DIR_HTML_RESULT.mkdir(parents=True, exist_ok=True)
    fetch_result_html_bulk(race_ids, max_workers=16)

    # ---------- â˜… Chrome å®Œå…¨çµ‚äº† â†’ GC --------------------------
    kill_zombie_chrome()
    import gc, time
    gc.collect(); time.sleep(1)                 # DLL è§£æ”¾å¾…ã¡
    # ------------------------------------------------------------

    # ---------- â‘¢ result.html â†’ DataFrame (å¤šãƒ—ãƒ­ã‚»ã‚¹) ----------
    df_result, skipped = parse_result_html_bulk_loky(
        race_ids,
        DIR_HTML_RESULT,
        max_workers=max(1, int(os.cpu_count() * CPU_RATIO))
    )
    if df_result.empty:
        logging.error("[TRAIN] result parse 0 ä»¶â€¦ä¸­æ–­"); return
    DIR_DATA.mkdir(exist_ok=True)
    df_result.to_pickle(PKL_RESULT)
    logging.info(f"[SAVE] df_result â†’ {PKL_RESULT}")
    if skipped:
        Path("skipped_result_ids.txt").write_text("\n".join(skipped), encoding="utf-8")
        logging.info(f"[SAVE] skipped_result_ids.txt ({len(skipped)})")

    # ---------- â‘£ horse ãƒšãƒ¼ã‚¸ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰  (Chrome ä½¿ç”¨) ------
    horse_ids = sorted(df_result["horse_id"].dropna()
                       .astype(str).loc[lambda s: s.str.len() == 10].unique())
    DIR_HTML_HORSE.mkdir(parents=True, exist_ok=True)
    scrape_horse_pages_bulk(horse_ids)

    # ---------- â˜… å†åº¦ Chrome å®Œå…¨çµ‚äº† â†’ GC ----------------------
    kill_zombie_chrome()
    gc.collect(); time.sleep(1)
    # ------------------------------------------------------------

    # ---------- â‘¤ horse ãƒ‘ãƒ¼ã‚¹ & ç‰¹å¾´é‡ (å¤šãƒ—ãƒ­ã‚»ã‚¹) ------------
    df_horse = parse_horse_data_all(
        horse_ids,
        max_workers=max(1, int(os.cpu_count() * CPU_RATIO))
    )
    df_horse.to_pickle(PKL_HORSE_ALL)

    # â‘¤-A åŒºé–“ã‚¿ã‚¤ãƒ ãƒ»è„šè³ª
    df_section = calc_section_features(df_horse, df_result)
    df_section.to_pickle(PKL_SECTION)

    print("[DEBUG] df_section columns:", df_section.columns.tolist())
    print("[DEBUG] df_section shape:", df_section.shape)

    df_section.to_pickle(PKL_SECTION)

    # â‘¤-B past5
    df_p5 = calc_past5_features(df_horse)
    df_p5.to_pickle(PKL_PAST5)

    # â‘¤-C é¨æ‰‹é©æ€§
    df_jk = calc_jockey_features(df_result)
    df_jk.to_pickle(PKL_JOCKEY)

    # ---------- â‘¥ LightGBM å­¦ç¿’ ----------------------------------
    df_model = (df_result
                  .merge(df_p5,     on="horse_id",  how="left")
                  .merge(df_section,on="horse_id",  how="left")
                  .merge(df_jk,     on="jockey_id", how="left"))

    model = train_lightgbm(df_model)
    df_model["pred_score"] = model.predict(df_model[FEATS].fillna(0))

    # ---------- â‘¦ ROI ãƒã‚§ãƒƒã‚¯ -----------------------------------
    if ANSWER_CHECK_ENABLED:
        answer_check(df_model)

    logging.info("=== TRAIN å®Œäº† ===")


# %% =========================================================
#  PREDICT ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³  â€• Chrome ãƒ•ã‚§ãƒ¼ã‚ºã¨å¾Œç¶šå‡¦ç†ã‚’åˆ†é›¢
# ============================================================
def predict_pipeline():
    logging.info("=== PREDICT MODE ===")

    # ---------- â‘  race_id å–å¾— (Chrome ä½¿ç”¨) --------------------
    rids = scrape_race_ids_one_day(RACE_DATE)
    if not rids:
        logging.error(f"[PREDICT] race_id 0 ä»¶ ({RACE_DATE})"); return

    # ---------- â‘¡ shutuba HTML ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (Chrome ä½¿ç”¨) ------
    DIR_HTML_RACE.mkdir(parents=True, exist_ok=True)
    for rid in tqdm_notebook(rids, desc="shutuba html"):
        ensure_html(
            path=DIR_HTML_RACE / f"shutuba_{rid}.bin",
            url=f"https://race.netkeiba.com/race/shutuba.html?race_id={rid}",
            wait_xpaths=['//*[@class="Shutuba_Table"]'],
            no_images=True
        )

    # ---------- â˜… Chrome å®Œå…¨çµ‚äº† â†’ GC --------------------------
    kill_zombie_chrome()
    import gc, time
    gc.collect(); time.sleep(1)
    # ------------------------------------------------------------

    # ---------- â‘¢ shutuba è§£æ â†’ horse & jockey ID -------------
    dfs = []
    for rid in rids:
        fp = DIR_HTML_RACE / f"shutuba_{rid}.bin"
        if not fp.exists(): continue
        soup = BeautifulSoup(fp.read_bytes(), "lxml")
        tbl  = soup.find("table", class_="Shutuba_Table")
        if not tbl: continue

        df = pd.read_html(str(tbl))[0]

        # é¦¬ ID
        horse_ids = [re.search(r"/horse/(\d+)", a["href"]).group(1)
                     for a in tbl.find_all("a", href=re.compile(r"/horse/\d+"))]

        # é¨æ‰‹ ID
        jockey_ids = []
        for a in tbl.find_all("a", href=True):
            m = re.search(r"/jockey/(?:profile/|detail/)?(\d+)", a["href"])
            jockey_ids.append(m.group(1) if m else None)
        jockey_ids = jockey_ids[:len(horse_ids)] + [None]*(len(horse_ids)-len(jockey_ids))

        df = df.head(len(horse_ids))
        df["horse_id"]  = horse_ids
        df["jockey_id"] = jockey_ids[:len(df)]
        df["race_id"]   = rid
        dfs.append(df[["race_id", "horse_id", "jockey_id"]])

    if not dfs:
        logging.error("[PREDICT] shutuba parse 0"); return

    # ---------- â‘£ ç‰¹å¾´é‡çµåˆ ------------------------------------
    df_pred = pd.concat(dfs, ignore_index=True)
    df_pred = (df_pred
               .merge(pd.read_pickle(PKL_PAST5),   on="horse_id",  how="left")
               .merge(pd.read_pickle(PKL_SECTION), on="horse_id",  how="left")
               .merge(pd.read_pickle(PKL_JOCKEY),  on="jockey_id", how="left"))

    # ---------- â‘¤ äºˆæ¸¬ ------------------------------------------
    model = lgb.Booster(model_file=str(MODEL_FILE))
    df_pred["pred_score"] = model.predict(df_pred[FEATS].fillna(0))

    DIR_OUTPUT.mkdir(parents=True, exist_ok=True)
    out = DIR_OUTPUT / f"predict_{RACE_DATE}.csv"
    df_pred.to_csv(out, index=False, encoding="utf-8-sig")
    logging.info(f"[SAVE] â†’ {out}")

    # ---------- â‘¥ ä»»æ„ ROI ãƒã‚§ãƒƒã‚¯ ------------------------------
    if ANSWER_CHECK_ENABLED:
        answer_check(df_pred)

# ============================================================
# 12. main
# ============================================================
def main():
    train_pipeline() if TRAIN_MODE else predict_pipeline()

if __name__ == "__main__":
    main()
